{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55b34128-f43a-47ca-849c-49117c4f5c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from imblearn.over_sampling import RandomOverSampler\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from sklearn.metrics import classification_report, f1_score\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71936f0b-be4a-4e46-9295-ec33e0b3ebb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\user\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download NLTK Resources\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "37705b30-058a-4443-a9b6-1b7b1a55ab61",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load & Preprocess Data\n",
    "df = pd.read_csv(\"C:/Users/user/Downloads/tweet_emotions.csv/tweet_emotions.csv\")\n",
    "\n",
    "# Encode labels\n",
    "le = LabelEncoder()\n",
    "df['sentiment_encoded'] = le.fit_transform(df['sentiment'])\n",
    "\n",
    "# Cleaning functions\n",
    "lemmatizer_obj = WordNetLemmatizer()\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def clean_text(text):\n",
    "    if pd.isnull(text):\n",
    "        return \"\"\n",
    "    text = str(text).lower()\n",
    "    text = re.sub(r'@[\\w]*', '', text)\n",
    "    text = re.sub(r'https?://\\S+|www\\.\\S+', '', text)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def lemmatizer(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    lemmatized = [lemmatizer_obj.lemmatize(word) for word in tokens if word not in stop_words]\n",
    "    return ' '.join(lemmatized).strip()\n",
    "\n",
    "df['cleaned_text'] = df['content'].apply(clean_text).apply(lemmatizer)\n",
    "df = df[df['cleaned_text'].str.strip().astype(bool)].copy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "43a804c4-4302-4738-aed4-56e8b7556f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Balance the Classes\n",
    "\n",
    "X = df['cleaned_text'].astype(str)\n",
    "y = df['sentiment_encoded']\n",
    "\n",
    "X_df = pd.DataFrame({'text': X})\n",
    "y_df = pd.Series(y)\n",
    "\n",
    "oversampler = RandomOverSampler(random_state=42)\n",
    "X_resampled_df, y_resampled = oversampler.fit_resample(X_df, y_df)\n",
    "\n",
    "X = X_resampled_df['text']\n",
    "y = y_resampled\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8868339d-9ba6-49da-b7d8-c3783940f774",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train/Validation/Test Split\n",
    "X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.3, stratify=y, random_state=42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, stratify=y_temp, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d65c5ac8-229b-4626-a251-d371231d0132",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenization & Padding\n",
    "max_vocab_size = 10000\n",
    "max_sequence_length = 100\n",
    "\n",
    "tokenizer = Tokenizer(num_words=max_vocab_size, oov_token=\"<OOV>\")\n",
    "tokenizer.fit_on_texts(X_train)\n",
    "\n",
    "X_train_seq = tokenizer.texts_to_sequences(X_train)\n",
    "X_val_seq = tokenizer.texts_to_sequences(X_val)\n",
    "X_test_seq = tokenizer.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_val_pad = pad_sequences(X_val_seq, maxlen=max_sequence_length, padding='post')\n",
    "X_test_pad = pad_sequences(X_test_seq, maxlen=max_sequence_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa7680e2-e439-4cdb-8f9e-04669eacae5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "# Load GloVe 300D Embeddings\n",
    "\n",
    "embedding_index = {}\n",
    "with open(\"glove.6B.300d.txt\", encoding=\"utf-8\") as f:  \n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coeffs = np.asarray(values[1:], dtype=\"float32\")\n",
    "        embedding_index[word] = coeffs\n",
    "\n",
    "print(f\"Loaded {len(embedding_index)} word vectors.\")\n",
    "\n",
    "embedding_dim = 300  # 300D GloVe\n",
    "vocab_size = min(max_vocab_size, len(tokenizer.word_index) + 1)\n",
    "\n",
    "embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if i >= vocab_size:\n",
    "        continue\n",
    "    embedding_vector = embedding_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e945953c-5cef-44f3-a54d-7a877209023c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model with dropout=0.3, learning_rate=0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m422s\u001b[0m 337ms/step - accuracy: 0.2831 - loss: 2.1081 - val_accuracy: 0.5408 - val_loss: 1.3624\n",
      "Epoch 2/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m425s\u001b[0m 351ms/step - accuracy: 0.5622 - loss: 1.2998 - val_accuracy: 0.6145 - val_loss: 1.1219\n",
      "Epoch 3/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m405s\u001b[0m 335ms/step - accuracy: 0.6520 - loss: 1.0264 - val_accuracy: 0.6616 - val_loss: 0.9923\n",
      "Epoch 4/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m417s\u001b[0m 345ms/step - accuracy: 0.6958 - loss: 0.8798 - val_accuracy: 0.6961 - val_loss: 0.8904\n",
      "Epoch 5/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m415s\u001b[0m 343ms/step - accuracy: 0.7386 - loss: 0.7542 - val_accuracy: 0.7205 - val_loss: 0.8264\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 59ms/step\n",
      "\n",
      "Done: Dropout=0.3, LR=0.001\n",
      "Validation Accuracy: 0.7205\n",
      "Weighted F1-score: 0.7123\n",
      "\n",
      "Training model with dropout=0.3, learning_rate=0.0005\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m436s\u001b[0m 349ms/step - accuracy: 0.2281 - loss: 2.2584 - val_accuracy: 0.4846 - val_loss: 1.5561\n",
      "Epoch 2/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m349s\u001b[0m 289ms/step - accuracy: 0.4827 - loss: 1.5472 - val_accuracy: 0.5681 - val_loss: 1.2837\n",
      "Epoch 3/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m328s\u001b[0m 271ms/step - accuracy: 0.5756 - loss: 1.2640 - val_accuracy: 0.6109 - val_loss: 1.1493\n",
      "Epoch 4/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m330s\u001b[0m 273ms/step - accuracy: 0.6275 - loss: 1.0969 - val_accuracy: 0.6406 - val_loss: 1.0474\n",
      "Epoch 5/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m341s\u001b[0m 282ms/step - accuracy: 0.6633 - loss: 0.9869 - val_accuracy: 0.6612 - val_loss: 0.9926\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m28s\u001b[0m 52ms/step\n",
      "\n",
      "Done: Dropout=0.3, LR=0.0005\n",
      "Validation Accuracy: 0.6612\n",
      "Weighted F1-score: 0.6575\n",
      "\n",
      "Training model with dropout=0.5, learning_rate=0.001\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m380s\u001b[0m 302ms/step - accuracy: 0.2145 - loss: 2.2855 - val_accuracy: 0.4666 - val_loss: 1.5812\n",
      "Epoch 2/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m442s\u001b[0m 366ms/step - accuracy: 0.4578 - loss: 1.6003 - val_accuracy: 0.5620 - val_loss: 1.2907\n",
      "Epoch 3/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m451s\u001b[0m 373ms/step - accuracy: 0.5511 - loss: 1.3309 - val_accuracy: 0.6020 - val_loss: 1.1599\n",
      "Epoch 4/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m347s\u001b[0m 287ms/step - accuracy: 0.6081 - loss: 1.1642 - val_accuracy: 0.6358 - val_loss: 1.0669\n",
      "Epoch 5/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m315s\u001b[0m 260ms/step - accuracy: 0.6432 - loss: 1.0529 - val_accuracy: 0.6547 - val_loss: 1.0073\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m20s\u001b[0m 37ms/step\n",
      "\n",
      "Done: Dropout=0.5, LR=0.001\n",
      "Validation Accuracy: 0.6547\n",
      "Weighted F1-score: 0.6449\n",
      "\n",
      "Training model with dropout=0.5, learning_rate=0.0005\n",
      "\n",
      "Epoch 1/5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\Lib\\site-packages\\keras\\src\\layers\\core\\embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m456s\u001b[0m 371ms/step - accuracy: 0.1688 - loss: 2.4163 - val_accuracy: 0.4048 - val_loss: 1.7510\n",
      "Epoch 2/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m485s\u001b[0m 402ms/step - accuracy: 0.3782 - loss: 1.8168 - val_accuracy: 0.4994 - val_loss: 1.5078\n",
      "Epoch 3/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m369s\u001b[0m 305ms/step - accuracy: 0.4700 - loss: 1.5677 - val_accuracy: 0.5500 - val_loss: 1.3453\n",
      "Epoch 4/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m493s\u001b[0m 408ms/step - accuracy: 0.5306 - loss: 1.3949 - val_accuracy: 0.5817 - val_loss: 1.2360\n",
      "Epoch 5/5\n",
      "\u001b[1m1209/1209\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m484s\u001b[0m 400ms/step - accuracy: 0.5694 - loss: 1.2767 - val_accuracy: 0.6011 - val_loss: 1.1604\n",
      "\u001b[1m519/519\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m40s\u001b[0m 73ms/step\n",
      "\n",
      "Done: Dropout=0.5, LR=0.0005\n",
      "Validation Accuracy: 0.6011\n",
      "Weighted F1-score: 0.5940\n",
      "\n",
      "All Results:\n",
      "Dropout=0.3 | LR=0.001 → Val Acc: 0.7205, F1: 0.7123\n",
      "Dropout=0.3 | LR=0.0005 → Val Acc: 0.6612, F1: 0.6575\n",
      "Dropout=0.5 | LR=0.001 → Val Acc: 0.6547, F1: 0.6449\n",
      "Dropout=0.5 | LR=0.0005 → Val Acc: 0.6011, F1: 0.5940\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Bidirectional\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "\n",
    "# values to test\n",
    "dropout_rates = [0.3, 0.5]\n",
    "learning_rates = [0.001, 0.0005]\n",
    "\n",
    "results = []\n",
    "\n",
    "# Loop through combinations\n",
    "for dropout in dropout_rates:\n",
    "    for lr in learning_rates:\n",
    "        print(f\"\\nTraining model with dropout={dropout}, learning_rate={lr}\\n\")\n",
    "\n",
    "        # Build model with fixed structure but variable hyperparameters\n",
    "        model = Sequential([\n",
    "            Embedding(input_dim=vocab_size, output_dim=300,\n",
    "                      weights=[embedding_matrix], input_length=max_sequence_length, trainable=True),\n",
    "            \n",
    "            Bidirectional(LSTM(128, dropout=dropout, recurrent_dropout=dropout)),\n",
    "            Dense(64, activation='relu'),\n",
    "            Dropout(dropout),\n",
    "            Dense(len(le.classes_), activation='softmax')\n",
    "        ])\n",
    "\n",
    "        # Compile model\n",
    "        optimizer = Adam(learning_rate=lr)\n",
    "        model.compile(loss='sparse_categorical_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "\n",
    "        # Callbacks\n",
    "        es = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
    "\n",
    "        # Train model\n",
    "        history = model.fit(\n",
    "            X_train_pad, y_train,\n",
    "            validation_data=(X_val_pad, y_val),\n",
    "            epochs=5,\n",
    "            batch_size=64,\n",
    "            callbacks=[es],\n",
    "        )\n",
    "\n",
    "        # Evaluate model\n",
    "        y_val_pred = np.argmax(model.predict(X_val_pad), axis=-1)\n",
    "        f1 = f1_score(y_val, y_val_pred, average='weighted')\n",
    "        val_acc = history.history['val_accuracy'][-1]\n",
    "\n",
    "        print(f\"\\nDone: Dropout={dropout}, LR={lr}\")\n",
    "        print(f\"Validation Accuracy: {val_acc:.4f}\")\n",
    "        print(f\"Weighted F1-score: {f1:.4f}\")\n",
    "\n",
    "        results.append({\n",
    "            \"dropout\": dropout,\n",
    "            \"learning_rate\": lr,\n",
    "            \"val_accuracy\": val_acc,\n",
    "            \"f1_score\": f1\n",
    "        })\n",
    "\n",
    "# Summary of results\n",
    "print(\"\\nAll Results:\")\n",
    "for res in results:\n",
    "    print(f\"Dropout={res['dropout']} | LR={res['learning_rate']} → Val Acc: {res['val_accuracy']:.4f}, F1: {res['f1_score']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
